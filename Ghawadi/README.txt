# Project Title

This README describes the source code used as part of COMP90049 2018S2, Project deliverables.

## Package Components:

- SimilaritySpellCheck.py: This is the main program used for the experiment and evaluation. It uses wiki_misspell, wiki_correct, dict.txt and generates a csv file containing all statistics information about each of the misspelled entry and number of possible correct words predictions along with evaluation of whether the prediction matched the expected intended token from wiki_correct file.

- FetchAllMatches.py: This is an extension program to extend the analysis, it will generate a data dump csv file containing a list of all possible predictions for each of the enabled algorithms.

- Local_statistics_final_analysis.xlsx: This is built on top of the file generated by running "SimilaritySpellCheck.py". We added additional formulas to calculate the overall statistical information on the data set including the summarized evaluation matrix including precision and recall.


## how your programs work:

The program reads all entries from wiki_misspell.txt and measures the similarity of each word(token) from the file agains dict.txt and comes with the best prediction(s) of the possible correct word(s) to the misspelled word.
The program uses 5 different measures (Edit Distance, Soundex and N-Gram (UniGram, BiGram and 5-Gram)) and create a statistical matrix for each word, the best "similarity weight", "number of predictions (with same weight)", and a flag to indicate if the correct intended word (from wiki_correct.txt) was one of those predictions. This is done for each similarity measure set.
The output is put in CSV format for ease of opening in excel to do additional manipulation and calculations on the data.
So the Precision & Recall is calculated using excel formulas out from the output provided from this program.
This is done in this way to give flexibility to select a subset later on and recalculate the evaluation matrix without the need to rerun the program every time as it is time intensive (The file including the calculations is attached in the package as well "Local_statistics_final_analysis.xlsx").


### Prerequisites

- Python 3.3 and up
- Install extra resources packages as listed below


### External Resources

Following external libraries/resources has been used in the programs and need to be installed before successfully running the application:

* dataset (not included in the package assuming that it is available):
	- wiki_misspell.txt: extracted https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings 
	- wiki_correct.txt: a list of intended tokens for each entry in wiki_misspell
	- dict.txt: altered version of the data from https://github.com/dwyl/english-words

* pandas:
	- pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with structured and time series data both easy and intuitive.
	- To Install: pip install pandas
	- URL: https://pypi.org/project/pandas/

* python-Levenshtein: 
	- The Levenshtein Python C extension module contains functions for fast computation of Levenshtein (edit) distance, and other string similarity measures
	- To Install: pip install python-Levenshtein
	- URL: https://pypi.org/project/python-Levenshtein/
* ngram:
	- The NGram class extends the Python ‘set’ class with efficient fuzzy search for members by means of an N-gram similarity measure
	- To Install: pip install ngram
	- URL: https://pypi.org/project/ngram/

* jellyfish:
	- Jellyfish is a python library for doing approximate and phonetic matching of strings.
	- To Install: pip install jellyfish
	- URL: https://pypi.org/project/jellyfish/

## Running the tests:
** Please note that the program does take a long time to run especially when Ngram is enabled
1) Before running you may want to set the following variables (line 20 in SimilaritySpellCheck.py and line 21 in FetchAllMatches.py)
	- enableED: Set 1 to enable or 0 to disable this algorithm
	- enableNG: Set 1 to enable or 0 to disable this algorithm
	- enableSX: Set 1 to enable or 0 to disable this algorithm

2) SimilaritySpellCheck.py:
	- Upon running this program, an initial file gets created "Local_statistics_init.csv" with list of all entries from wiki_misspell.txt combined with corresponding wiki_correct.txt. Seeing this file just indicated the program started running.
	- After a certain set of records is processed an intermediate file gets created and keep getting updated so we can see the incremental data being processed (the records frequency refresh count is set in Line 244).
	- Once the program is complete a final file with all data "Local_statistics_final.csv" gets created.

3) FetchAllMatches.py:
	- This is an extension program after running it will create "datadump.csv" file contain the list of all possible predictions for each enabled method for further analysis.
	- It expected "Local_statistics_Summary.csv" to be available, which is the output from SimilaritySpellCheck.py (I changed the name in order not to overwrite the original file by mistake during testing and be able to change the source dataset based for retesting on smaller set.


## Authors

* Ghawady Ehmaid (Student ID 983899)


## Acknowledgments

* Thanks to Jeremy Nicholson for providing the initial sample code based on which this program was built upon and enhanced