{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2020 Semester 1\n",
    "-----\n",
    "## Project 1: Understanding Student Success with Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Youngsun Kim\n",
    "###### Python version: 3.7\n",
    "###### Submission deadline: 11am, Wed 22 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Project 1 submission. \n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Global variables\n",
    "data_total = [0]\n",
    "d_feature = [0]\n",
    "d_class = [0]\n",
    "count_df = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format\n",
    "def load_data(filepath):\n",
    "    global data_total, d_feature, d_class\n",
    "    data_total = pd.read_csv(filepath, sep=',')\n",
    "    d_feature = data_total.iloc[:, 0:29]\n",
    "    d_class = data_total['Grade']\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function should split a data set into a training set and hold-out test set\n",
    "# res[0] :  training data [0-518] 80% of the total data\n",
    "# res[1] : test data [519-648] 20% of the total data\n",
    "def split_data():\n",
    "    arr_train = list(range(0, 519))\n",
    "    arr_test = list(range(519, 649))\n",
    "    res = [0]\n",
    "    res[0] = arr_train\n",
    "    res.append(arr_test)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(train_index):\n",
    "    global d_feature\n",
    "    global d_class\n",
    "    global data_total\n",
    "\n",
    "    train_inst = data_total.iloc[train_index]\n",
    "\n",
    "    # region [make count table]\n",
    "    global count_df\n",
    "\n",
    "    count_df[0] = train_inst.groupby(['Grade', 'school']).size().reset_index(name='count')\n",
    "\n",
    "    # test\n",
    "    # print(\"[count_school]\"); print(count_df[0])\n",
    "    # access to conditional value\n",
    "    # res = count_df[0].loc[(count_df[0]['Grade'] == 'A') & (count_df[0]['school'] == 'GP')].at[0,'count']\n",
    "    # print(\"res\", res)\n",
    "    ##\n",
    "    count_df.append(train_inst.groupby(['Grade', 'sex']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'address']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'famsize']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Pstatus']).size().reset_index(name='count'))\n",
    "\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Medu']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Fedu']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Mjob']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Fjob']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'reason']).size().reset_index(name='count'))\n",
    "\n",
    "    count_df.append(train_inst.groupby(['Grade', 'guardian']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'traveltime']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'studytime']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'failures']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'schoolsup']).size().reset_index(name='count'))\n",
    "\n",
    "    count_df.append(train_inst.groupby(['Grade', 'famsup']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'paid']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'activities']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'nursery']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'higher']).size().reset_index(name='count'))\n",
    "\n",
    "    count_df.append(train_inst.groupby(['Grade', 'internet']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'romantic']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'famrel']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'freetime']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'goout']).size().reset_index(name='count'))\n",
    "\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Dalc']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'Walc']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'health']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby(['Grade', 'absences']).size().reset_index(name='count'))\n",
    "    count_df.append(train_inst.groupby('Grade').size().reset_index(name='count'))  # 29\n",
    "\n",
    "    # print(count_df)\n",
    "    # test\n",
    "\n",
    "    # print(\"모든 count table\")\n",
    "    # print(count_df)\n",
    "    # print(\"[count_Grade]\")\n",
    "    # print(count_df[29])\n",
    "    # print(\"[Grade at : ]\", count_df[29].iat[0, 1])\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model\n",
    "def predict(test_index):\n",
    "    GradeArr = [\"A+\", \"A\", \"B\", \"C\", \"D\", \"F\"]\n",
    "    AttbArr = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian',\n",
    "               'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher',\n",
    "               'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
    "\n",
    "    test_inst = data_total.iloc[test_index]\n",
    "    grade_tot = count_df[29].sum(numeric_only=True).iat[0]\n",
    "    predicted_class = [0]\n",
    "    i = 0\n",
    "\n",
    "    for inst in test_inst.iterrows():\n",
    "\n",
    "        max_y = float('-inf')  # minimum int\n",
    "        est_res = \"\"\n",
    "\n",
    "        for g in GradeArr:\n",
    "\n",
    "            grade_cnt = count_df[29].loc[count_df[29]['Grade'] == g].iat[0, 1]\n",
    "            est_y = np.log(grade_cnt / grade_tot)\n",
    "\n",
    "            for att_idx in range(29):\n",
    "                df_condition = count_df[att_idx].loc[\n",
    "                    (count_df[att_idx]['Grade'] == g) & (count_df[att_idx][AttbArr[att_idx]] == inst[1].array[att_idx])]\n",
    "\n",
    "                if df_condition.empty == True:\n",
    "                    cnt_test = 0  # 0 - smoothing?\n",
    "                else:\n",
    "                    cnt_test = df_condition.iat[0, 2]\n",
    "\n",
    "                if grade_cnt <=0:\n",
    "                    est_y += 0\n",
    "                if cnt_test <=0:\n",
    "                    est_y = float('-inf')+1\n",
    "                else:\n",
    "                    est_y += np.log(cnt_test / grade_cnt)\n",
    "\n",
    "            if est_y >= max_y:\n",
    "                max_y = est_y\n",
    "                est_res = g\n",
    "\n",
    "        if i == 0:\n",
    "            predicted_class[0] = est_res\n",
    "        else:\n",
    "            predicted_class.append(est_res)\n",
    "        i += 1\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#For Q5 Only, commented out at the main function\n",
    "def predict_fair(test_index):\n",
    "    GradeArr = [\"A+\", \"A\", \"B\", \"C\", \"D\", \"F\"]\n",
    "    AttbArr = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian',\n",
    "               'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher',\n",
    "               'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
    "\n",
    "    test_inst = data_total.iloc[test_index]\n",
    "    grade_tot = count_df[29].sum(numeric_only=True).iat[0]\n",
    "    predicted_class = [0]\n",
    "    i = 0\n",
    "\n",
    "    for inst in test_inst.iterrows():\n",
    "\n",
    "        max_y = float('-inf')  # minimum int\n",
    "        est_res = \"\"\n",
    "\n",
    "        for g in GradeArr:\n",
    "\n",
    "            grade_cnt = count_df[29].loc[count_df[29]['Grade'] == g].iat[0, 1]\n",
    "            est_y = np.log(grade_cnt / grade_tot)\n",
    "\n",
    "\n",
    "            for att_idx in range(29-7):\n",
    "\n",
    "                df_condition = count_df[att_idx].loc[\n",
    "                    (count_df[att_idx]['Grade'] == g) & (count_df[att_idx][AttbArr[att_idx]] == inst[1].array[att_idx])]\n",
    "\n",
    "                if df_condition.empty == True:\n",
    "                    cnt_test = 0  # 0 - smoothing?\n",
    "                else:\n",
    "                    cnt_test = df_condition.iat[0, 2]\n",
    "\n",
    "                # remove 7 features 1 4 5 6 7 8 10\n",
    "                if att_idx == 1 or att_idx == 4 or att_idx == 5 or att_idx == 6 or att_idx == 7 or att_idx == 8 or att_idx == 10 or grade_cnt == 0 or grade_cnt == 0.0:\n",
    "                    est_y += 0\n",
    "                else:\n",
    "                    est_y += np.log(cnt_test / grade_cnt)\n",
    "\n",
    "            if est_y >= max_y:\n",
    "                max_y = est_y\n",
    "                est_res = g\n",
    "\n",
    "        if i == 0:\n",
    "            predicted_class[0] = est_res\n",
    "        else:\n",
    "            predicted_class.append(est_res)\n",
    "        i += 1\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "def evaluate(test_index, prd_res):\n",
    "    real_res = d_class.iloc[test_index]\n",
    "    tot_cnt = real_res.size\n",
    "    i = 0\n",
    "    correct_cnt = 0\n",
    "\n",
    "    for real in real_res:\n",
    "        if real == prd_res[i]:\n",
    "            correct_cnt += 1\n",
    "            # print(\"correct index: \", i)\n",
    "        # else : print(\"incorrect index: \", i)\n",
    "        i += 1\n",
    "\n",
    "    accr = correct_cnt / tot_cnt\n",
    "\n",
    "    return accr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Accuracy:  0.18461538461538463\n",
      "1 / 65 Processing...\n",
      "2 / 65 Processing...\n",
      "3 / 65 Processing...\n",
      "4 / 65 Processing...\n",
      "5 / 65 Processing...\n",
      "6 / 65 Processing...\n",
      "7 / 65 Processing...\n",
      "8 / 65 Processing...\n",
      "9 / 65 Processing...\n",
      "10 / 65 Processing...\n",
      "11 / 65 Processing...\n",
      "12 / 65 Processing...\n",
      "13 / 65 Processing...\n",
      "14 / 65 Processing...\n",
      "15 / 65 Processing...\n",
      "16 / 65 Processing...\n",
      "17 / 65 Processing...\n",
      "18 / 65 Processing...\n",
      "19 / 65 Processing...\n",
      "20 / 65 Processing...\n",
      "21 / 65 Processing...\n",
      "22 / 65 Processing...\n",
      "23 / 65 Processing...\n",
      "24 / 65 Processing...\n",
      "25 / 65 Processing...\n",
      "26 / 65 Processing...\n",
      "27 / 65 Processing...\n",
      "28 / 65 Processing...\n",
      "29 / 65 Processing...\n",
      "30 / 65 Processing...\n",
      "31 / 65 Processing...\n",
      "32 / 65 Processing...\n",
      "33 / 65 Processing...\n",
      "34 / 65 Processing...\n",
      "35 / 65 Processing...\n",
      "36 / 65 Processing...\n",
      "37 / 65 Processing...\n",
      "38 / 65 Processing...\n",
      "39 / 65 Processing...\n",
      "40 / 65 Processing...\n",
      "41 / 65 Processing...\n",
      "42 / 65 Processing...\n",
      "43 / 65 Processing...\n",
      "44 / 65 Processing...\n",
      "45 / 65 Processing...\n",
      "46 / 65 Processing...\n",
      "47 / 65 Processing...\n",
      "48 / 65 Processing...\n",
      "49 / 65 Processing...\n",
      "50 / 65 Processing...\n",
      "51 / 65 Processing...\n",
      "52 / 65 Processing...\n",
      "53 / 65 Processing...\n",
      "54 / 65 Processing...\n",
      "55 / 65 Processing...\n",
      "56 / 65 Processing...\n",
      "57 / 65 Processing...\n",
      "58 / 65 Processing...\n",
      "59 / 65 Processing...\n",
      "60 / 65 Processing...\n",
      "61 / 65 Processing...\n",
      "62 / 65 Processing...\n",
      "63 / 65 Processing...\n",
      "64 / 65 Processing...\n",
      "65 / 65 Done!\n",
      "[Cross-validation Result]\n",
      "[Running Time]:  132.3428509235382\n",
      "[N_SPLIT]:  65\n",
      "[Accuracy Array]:  [0.1, 0.2, 0.5, 0.1, 0.0, 0.6, 0.3, 0.2, 0.4, 0.1, 0.3, 0.6, 0.0, 0.2, 0.3, 0.1, 0.2, 0.3, 0.2, 0.3, 0.2, 0.2, 0.3, 0.3, 0.4, 0.1, 0.3, 0.2, 0.3, 0.3, 0.2, 0.3, 0.4, 0.1, 0.1, 0.0, 0.3, 0.0, 0.2, 0.2, 0.1, 0.1, 0.4, 0.2, 0.2, 0.3, 0.4, 0.4, 0.2, 0.2, 0.2, 0.3, 0.2, 0.1, 0.4, 0.0, 0.1, 0.2, 0.3, 0.2, 0.3, 0.1, 0.1, 0.2, 0.2222222222222222]\n",
      "[Averaged Accuracy]:  0.228034188034188\n"
     ]
    }
   ],
   "source": [
    "# region [Main]\n",
    "load_data('./Data/student.csv')\n",
    "\n",
    "# region [HOLD-OUT // SPLIT 80:20 // Accuracy : 0.34]\n",
    "res = split_data()\n",
    "train(res[0])\n",
    "prd_res = predict(res[1])  # returns predicted array (test instances')\n",
    "accr_res = evaluate(res[1], prd_res)\n",
    "print(\"Holdout Accuracy: \", accr_res)\n",
    "# endregion\n",
    "\n",
    "\n",
    "# region [CROSS-VALIDATION // Accuracy : 0.49]\n",
    "start_time = time.time()\n",
    "\n",
    "N_SPLIT = 65\n",
    "kf = KFold(n_splits=N_SPLIT, random_state=0)  # n_fold=65, 649\n",
    "\n",
    "i = 0\n",
    "accr_df = [0]\n",
    "\n",
    "for train_index, test_index in kf.split(data_total):\n",
    "    train(train_index)\n",
    "    prd_res = predict(test_index)\n",
    "\n",
    "    #(Q5)Fair way\n",
    "    #prd_res = predict_fair(test_index)\n",
    "\n",
    "    accr_res = evaluate(test_index, prd_res)\n",
    "    if i == 0:\n",
    "        accr_df[i] = accr_res\n",
    "    else:\n",
    "        accr_df.append(accr_res)\n",
    "        \n",
    "    if i == 64 : print(i+1,\"/\",N_SPLIT,\"Done!\")\n",
    "    else : print(i+1,\"/\",N_SPLIT,\"Processing...\")\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"[Cross-validation Result]\")\n",
    "print(\"[Running Time]: \", (time.time() - start_time))\n",
    "print(\"[N_SPLIT]: \", N_SPLIT)\n",
    "print(\"[Accuracy Array]: \", accr_df)\n",
    "print(\"[Averaged Accuracy]: \", np.mean(accr_df))\n",
    "\n",
    "# endregion\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Questions (you may respond in a cell or cells below):\n",
    "You should respond to Question 1 and two additional questions of your choice.\n",
    " A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "### Question 1: Naive Bayes Concepts and Implementation\n",
    "- a Explain the ‘naive’ assumption underlying Naive Bayes. (1) Why is it necessary? (2) Why can it be problematic? Link your discussion to the features of the students data set. [no programming required]\n",
    "- b Implement the required functions to load the student dataset, and estimate a Naive Bayes model. Evaluate the resulting classifier using the hold-out strategy, and measure its performance using accuracy.\n",
    "- c What accuracy does your classifier achieve? Manually inspect a few instances for which your classifier made correct predictions, and some for which it predicted incorrectly, and discuss any patterns you can find.\n",
    "\n",
    "[1-a-(1)] The ‘naive’ assumption underlying Naive Bayes is the conditional independence assumption which is assuming that all the features are assumed to be independent. This assumption is necessary to make the model more feasible. As it is shown below, this assumption makes the joint model into a ‘countable’ and ‘separated’ probabilistic model.\n",
    "P(x1, x2, ..., xM|y)P(y) => P(x1|y)P(x2|y)...P(xM|y)P(y)\n",
    "\n",
    "\n",
    "[1-a-(2)] For instance, both parents’ education and jobs can affect to choose a school and the area where the family lives. Real-world data’s features are related to each other. For another example, travel time must be affected by the distance between their schools and address(home) and it can affect study time as well. Also, these features are contributing to the result (Grade) with different weights. The conditional independence assumption is unrealistic as it ignores the correlations between features.\n",
    "\n",
    "\n",
    "[1-b] In my code, “# region [Main]” has “# region [HOLD-OUT]” and “# region [CROSS-VALIDATION]”. If we look at HOLD-OUT Region for this question, it calls\n",
    "\n",
    "\tload_data('./Data/student.csv') \n",
    "\t\n",
    "    res = split_data() : Since using data that has been used for training to test cannot evaluate the classifier’s performance, I used the hold-out method which separates test and training data. Also I used the commonly used rate 80:20 in the function. Thus the training data range becomes [0-518] and the test data becomes [519-648]. At here, I assumed that the both test and training data sets have the same class distribution.\n",
    "\t\n",
    "    train(res[0])\n",
    "\t\n",
    "    prd_res = predict(res[1]) : This function returns predicted class array “prd_res”. For estimated y, I moved the equation from [P(y) * πP(x|y)] to log space [logP(y) + ∑logP(x|y)]. Even if there is an unseen case, it would not make the whole equation 0 by multiplying it. Therefore, I did not use any smoothing strategy.\n",
    "\t\n",
    "    accr_res = evaluate(res[1], prd_res) : For evaluation, I used accuracy. If this classifier would be used as to accept students for a college, I could consider implementing the Precision method. As calculating A/A+ -> A/A+ as True Positive and A/A+ -> Lower than B as False Positive.\n",
    "\n",
    "[1-c] I got an accuracy of 0.34.\n",
    "I analyzed instances on an excel file. View online or please find the attached file. https://docs.google.com/spreadsheets/d/10nAbiyw6Uttmpd9PhWgdBuPxSI1iPDpHMKVaGKjizs8/edit?usp=sharing\n",
    "\n",
    "As the file shows, incorrect results tend to show two characteristics. Firstly, if there is a zero counted feature (unseen feature) for a specific attribute, it tends to classified as a class that has the biggest possibility number in that attribute. \n",
    "For example, if you see the cells [O 27:33] of the excel file, the attribute “failures” has unseen feature A+ and B: \n",
    "    # P(failures=low|A+) = 0, P(failures=low|B) = 0\n",
    "    # P(failures=low|F) = 0.35 \n",
    "In that attribute, P(failures=low|F) = 0.35 has the largest possibility. And this instance is classified as F. \n",
    "For another example, cells [AC 44:50] show as below. It has an unseen feature and the largest possibility case is A in attribute “health”. And it is classified as A.\n",
    "\t# P(health=4|A+) = 0\n",
    "    # P(health=4|A) = 0.22\n",
    "\n",
    "Secondly, in certain attributes, the wrongly classified class tends to show a large number of observations. And especially if the class has a small number of the total cases, it leads the probability makes a prominent peak (relatively large number / relatively small number). For example,\n",
    "\t# P(A) = 50, P(F) = 82 \n",
    "At [B 55:60], these Incorrectly estimated classes have a small total number.\n",
    "\t# P(school=MS|F) = P(58/82)  \n",
    "At [B 31:33], P(MS|real class)-P(MS|estimated class) = 0.3\n",
    "0.3 is a big difference and it gives “F” win.\n",
    "\t# P(famsize=GT3|F) = P(66/82)  \n",
    "\t\tAt [E 31:33], this case has the big difference = 0.11 again.\n",
    "    # [L50][P50][V50][X50][Y50] cells show that P(estimated case A) – P(real case B) has a big difference and it is estimated as A.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 3: Training Strategies \n",
    "There are other evaluation strategies, which tend to be preferred over the hold-out strategy you implemented in Question 1.\n",
    "- a Select one such strategy, (i) describe how it works, and (ii) explain why it is preferable over hold-out evaluation. [no programming required]\n",
    "- b Implement your chosen strategy from Question 3a, and report the accuracy score(s) of your classifier under this strategy. Compare your outcomes against your accuracy score in Question 1, and explain your observations in the context of your response to question 3a.\n",
    "\n",
    "[3-a-(i)] The cross-validation strategy is preferred over the hold-out. It divides N observation into k sets, and each set has N/k observation. A single set is to be a test data set and k-1 sets are training data sets. It iterates k times and uses each of the k sets as a test data set and the rest of them as training data sets. \n",
    "[3-a-(ii)] This strategy uses every instance as a test instance and a training instance. It can help to solve overfitting, selection bias and fluctuation or error by using all the instances as a training and test instance.\n",
    "\n",
    "[3-b]\n",
    "I implemented the cross-validation strategy in the main area of my code. The results are as below.\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "N_SPLIT:  5\n",
    "\n",
    "Running time:  130 sec\n",
    "\n",
    "[Accuracy Array]: [0.35, 0.53, 0.53, 0.46, 0.51]\n",
    "\n",
    "[Averaged Accuracy]:  0.48\n",
    "-------------------------------------------------------------------\n",
    "N_SPLIT:  10\n",
    "\n",
    "Running time:  136 sec\n",
    "\n",
    "[Accuracy Array]: [0.28, 0.48, 0.55, 0.48, 0.51, 0.45, 0.45, 0.45, 0.54, 0.5]\n",
    "\n",
    "[Averaged Accuracy]:  0.47\n",
    "-------------------------------------------------------------------\n",
    "N_SPLIT:  65\n",
    "\n",
    "Running time:  133 sec\n",
    "\n",
    "[Averaged Accuracy]:  0.49\n",
    "-------------------------------------------------------------------\n",
    "Running time:  176\n",
    "\n",
    "N_SPLIT:  649\n",
    "\n",
    "[Averaged Accuracy]:  0.48\n",
    "-------------------------------------------------------------------\n",
    "I tried 4 cases with N_SPLIT = 5,10,65 and 649(leave-one-out). Each case showed accuracy from 0.47 to 0.49 which is relatively similar to each other. And when we compare the result with the Hold-out’s accuracy 0.34, we can see that 0.34 was a biased result just by the selected test set. The N_SPLIT=10 case explains this problem. The accuracy result [0.28, 0.48, 0.55, 0.48, 0.51, 0.45, 0.45, 0.45, 0.54, 0.5] has 0.28 and it shows that if I just selected that set as a test set for the hold-out strategy, I would get 0.28 for a result and that would be very biased and not an accurate result. In conclusion, the cross-validation can help to mitigate bias and get more accurate evaluation result by averaging multiple results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 5: Bias and Fairness in Student Success Prediction\n",
    "As machine learning practitioners, we should be aware of possible ethical considerations around the\n",
    "applications we develop. The classifier you developed in this assignment could for example be used\n",
    "to classify college applicants into admitted vs not-admitted – depending on their predicted grade.\n",
    "- a Discuss ethical problems which might arise in this application and lead to unfair treatment of the applicants. Link your discussion to the set of features provided in the students data set. [no programming required]\n",
    "- b Select ethically problematic features from the data set and remove them from the data set. Use your own judgment (there is no right or wrong), and document your decisions. Train your Naive Bayes classifier on the resulting data set containing only ‘unproblematic’ features. How does the performance change in comparison to the full classifier?\n",
    "- c The approach to fairness we have adopted is called “fairness through unawareness” – we simply deleted any questionable features from our data. Removing all problematic features does not guarantee a fair classifier. Can you think of reasons why removing problematic features is not enough? [no programming required]\n",
    "\n",
    "[5-a] Accepting students based on their gender or their parent’s status (since those features affect calculating Grades in this classifier) can cause discrimination. Features “M/Fjob, M/Fedu, guardian, Pstatus” can lead a consequence that the college selects students based on the presence of parents, their financial status and their education level. Also, the feature “sex” can add a gender preference.\n",
    "\n",
    "\n",
    "[5-b] At the function predict_fair() in my code, I removed 7 feature columns as I mentioned at [5-a]. It showed an accuracy result of 0.41 which is 0.08 under compared to the original result 0.49. As removing 7 attributes out of 29, the classifier got a deterioration by 16 per cent ((0.49-0.41)*100/0.49).\n",
    "\n",
    "\n",
    "[5-c] First of all, excluding the controversial feature can be another discrimination. For instance, in our dataset, Female has 44 and male has 18 for A and A+ Grade. If we simply get rid of this observation, it would not be fair to female students. Also, it is ignoring real-life data with social bias and it’s not reflecting the truth truly. Secondly, even if we get rid of the problematic attribute, by ‘red-lining effect’[1], classifiers detect other features that correlate with the problematic attribute, learn the pattern and make a similar estimation. In the paper[1], the income classifier showed a similar result even without the sex attribute. The classifier used sex-correlated other features and discriminated indirectly. In conclusion, removing possible-discriminating features cannot solve the unfairness of machine learning completely.\n",
    "\n",
    "\n",
    "[Reference]\n",
    "[1] Toon Calders · Sicco Verwer. Three naive Bayes approaches for discrimination-free classification. [Online]: https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}